{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_LlXHYcmRaC"
      },
      "source": [
        "# Reduce on Plateau Learning Rate Scheduler\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/main/examples/contrib/reduce_on_plateau.ipynb)\n",
        "\n",
        "In this notebook, we explore the power of {py:func}`optax.contrib.reduce_on_plateau` scheduler, that reduces the learning rate when a metric has stopped improving. We will be solving a classification task by training a simple Multilayer Perceptron (MLP) on the fashion MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cu0kFNrnJj7"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from optax import contrib\n",
        "from optax import tree_utils as otu\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Show on which platform JAX is running.\n",
        "print(\"JAX running on\", jax.devices()[0].platform.upper())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLz5ue5BSSF0"
      },
      "source": [
        "## Data and model setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZej3FcOhuRE"
      },
      "source": [
        "Fashion MNIST is a dataset of 28x28 grayscale image, associated with a label from 10 classes. We now load the dataset using `tensorflow_datasets`, apply min-max normalization to the images, shuffle the data in the train set and create batches of size `BATCH_SIZE`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mRPJJftSSF0"
      },
      "outputs": [],
      "source": [
        "# @markdown Number of samples in each batch:\n",
        "BATCH_SIZE = 128  # @param{type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZ0paOehHWg"
      },
      "outputs": [],
      "source": [
        "(train_loader, test_loader), info = tfds.load(\n",
        "    \"fashion_mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
        ")\n",
        "\n",
        "min_max_norm = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\n",
        "train_loader = train_loader.map(min_max_norm)\n",
        "test_loader = test_loader.map(min_max_norm)\n",
        "\n",
        "NUM_CLASSES = info.features[\"label\"].num_classes\n",
        "IMG_SIZE = info.features[\"image\"].shape\n",
        "\n",
        "train_loader_batched = train_loader.shuffle(\n",
        "    buffer_size=10_000, reshuffle_each_iteration=True\n",
        ").batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkLaC2MlbAqa"
      },
      "source": [
        "The data is ready! Next let's define a model. Optax is agnostic to which (if any) neural network library is used. Here we use Flax to implement a simple MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RppusWrcaXzX"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"A simple multilayer perceptron model for image classification.\"\"\"\n",
        "  hidden_sizes: Sequence[int] = (1000, 1000)\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # Flattens images in the batch.\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=self.hidden_sizes[0])(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=self.hidden_sizes[1])(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=NUM_CLASSES)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKOi55MgdPyp"
      },
      "outputs": [],
      "source": [
        "net = MLP()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def loss_accuracy(params, data):\n",
        "  \"\"\"Computes loss and accuracy over a mini-batch.\n",
        "\n",
        "  Args:\n",
        "    params: parameters of the model.\n",
        "    data: tuple of (inputs, labels).\n",
        "\n",
        "  Returns:\n",
        "    loss: float\n",
        "  \"\"\"\n",
        "  inputs, labels = data\n",
        "  logits = net.apply({\"params\": params}, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=labels\n",
        "  ).mean()\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
        "  return loss, {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eB2dhIpjTIi"
      },
      "source": [
        "Next we initialize network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBnbq7gui34L"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "fake_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\n",
        "init_params = net.init({\"params\": rng}, fake_data)[\"params\"]\n",
        "\n",
        "\n",
        "def dataset_stats(params, data_loader):\n",
        "  \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n",
        "  all_accuracy = []\n",
        "  all_loss = []\n",
        "  for batch in data_loader.as_numpy_iterator():\n",
        "    batch_loss, batch_aux = loss_accuracy(params, batch)\n",
        "    all_loss.append(batch_loss)\n",
        "    all_accuracy.append(batch_aux[\"accuracy\"])\n",
        "  return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xumh2_JaSSF0"
      },
      "source": [
        "## Reduce on test loss plateau\n",
        "\n",
        "Here we consider an implementation that reduces the learning rate according to the test loss value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXNiinQUSd4c"
      },
      "outputs": [],
      "source": [
        "# @markdown Total number of epochs to train for:\n",
        "N_EPOCHS = 50  # @param{type:\"integer\"}\n",
        "# @markdown The base learning rate for the optimizer:\n",
        "LEARNING_RATE = 0.01  # @param{type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwd_ccHpSSF0"
      },
      "outputs": [],
      "source": [
        "# @markdown Number of epochs with no improvement after which learning rate will be reduced:\n",
        "PATIENCE = 5  # @param{type:\"integer\"}\n",
        "# @markdown Number of epochs to wait before resuming normal operation after the learning rate reduction:\n",
        "COOLDOWN = 0  # @param{type:\"integer\"}\n",
        "# @markdown Factor by which to reduce the learning rate:\n",
        "FACTOR = 0.5  # @param{type:\"number\"}\n",
        "# @markdown Relative tolerance for measuring the new optimum:\n",
        "RTOL = 1e-4  # @param{type:\"number\"}\n",
        "# @markdown Number of iterations to accumulate an average value:\n",
        "ACCUMULATION_SIZE = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPnR-3AbiEVG"
      },
      "source": [
        "Here we initialize the solver. Note that during the training process, the learning rate of the solver remains unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rU82drhaYM"
      },
      "outputs": [],
      "source": [
        "solver = optax.adam(LEARNING_RATE)\n",
        "solver_state = solver.init(init_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0dhka8X4mYI"
      },
      "source": [
        "In the next cell, we initialize the `contrib.reduce_on_plateau` scheduler, which reduces learning rate when a monitored metric has stopped improving. We will be using this scheduler to scale the updates, produced by the regular Adam optimizer.\n",
        "\n",
        "Additionally, the code initializes the state for the scheduler by calling `transform.init(parameters)`. Note that the initial learning rate for the scheduler is not explicitly set, so it will default to 1.0, which means there will be no scaling of the learning rate initially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDxQPVKA4iXC"
      },
      "outputs": [],
      "source": [
        "transform = contrib.reduce_on_plateau(\n",
        "    patience=PATIENCE,\n",
        "    cooldown=COOLDOWN,\n",
        "    factor=FACTOR,\n",
        "    rtol=RTOL,\n",
        "    accumulation_size=ACCUMULATION_SIZE\n",
        "    )\n",
        "\n",
        "# Creates initial state for `contrib.reduce_on_plateau` transformation.\n",
        "transform_state = transform.init(init_params)\n",
        "transform_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6GWNJf0XTY"
      },
      "source": [
        "The next cell trains the model for `N_EPOCHS` epochs. At the end of each epoch, the learning rate scaling value is updated based on the loss computed on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeQr0urBjoDj"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, solver_state, transform_state, batch):\n",
        "  \"\"\"Performs a one step update.\"\"\"\n",
        "  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n",
        "      params, batch\n",
        "  )\n",
        "  # Computes updates scaled by the learning rate that was used to initialize\n",
        "  # the `solver`.\n",
        "  updates, solver_state = solver.update(grad, solver_state, params)\n",
        "  # Scales updates, produced by `solver`, by the scaling value.\n",
        "  updates = otu.tree_scalar_mul(transform_state.scale, updates)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, solver_state, loss, aux\n",
        "\n",
        "params = init_params\n",
        "\n",
        "# Computes metrics at initialization.\n",
        "train_stats = dataset_stats(params, test_loader_batched)\n",
        "train_accuracy = [train_stats[\"accuracy\"]]\n",
        "train_losses = [train_stats['loss']]\n",
        "\n",
        "test_stats = dataset_stats(params, test_loader_batched)\n",
        "test_accuracy = [test_stats[\"accuracy\"]]\n",
        "test_losses = [test_stats[\"loss\"]]\n",
        "\n",
        "params = init_params\n",
        "lr_scale_history = [transform_state.scale]\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_accuracy_epoch = []\n",
        "  train_losses_epoch = []\n",
        "\n",
        "  for train_batch in train_loader_batched.as_numpy_iterator():\n",
        "    params, solver_state, train_loss, train_aux = train_step(\n",
        "        params, solver_state, transform_state, train_batch\n",
        "    )\n",
        "    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n",
        "    train_losses_epoch.append(train_loss)\n",
        "\n",
        "  mean_train_accuracy = np.mean(train_accuracy_epoch)\n",
        "  mean_train_loss = np.mean(train_losses_epoch)\n",
        "\n",
        "  # Adjusts the learning rate scaling value using the loss computed on the\n",
        "  # test set.\n",
        "  _, transform_state = transform.update(\n",
        "      updates=params, state=transform_state, value=test_stats[\"loss\"]\n",
        "  )\n",
        "  lr_scale_history.append(transform_state.scale)\n",
        "\n",
        "  train_accuracy.append(mean_train_accuracy)\n",
        "  train_losses.append(mean_train_loss)\n",
        "\n",
        "  test_stats = dataset_stats(params, test_loader_batched)\n",
        "  test_accuracy.append(test_stats[\"accuracy\"])\n",
        "  test_losses.append(test_stats[\"loss\"])\n",
        "\n",
        "  test_stats = dataset_stats(params, test_loader_batched)\n",
        "  test_accuracy.append(test_stats[\"accuracy\"])\n",
        "  test_losses.append(test_stats[\"loss\"])\n",
        "\n",
        "  print(\n",
        "      f\"Epoch {epoch + 1}/{N_EPOCHS}, mean train accuracy:\"\n",
        "      f\" {mean_train_accuracy}, lr scale: {transform_state.scale}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRdj9FORTNu3"
      },
      "outputs": [],
      "source": [
        "def plot(\n",
        "    lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy\n",
        "  ):\n",
        "  plt.rcParams[\"figure.figsize\"] = (20, 4.5)\n",
        "  plt.rcParams.update({\"font.size\": 18})\n",
        "\n",
        "  fig, axs = plt.subplots(ncols=5)\n",
        "\n",
        "  axs[0].plot(lr_scale_history[1:], lw=3)\n",
        "  axs[0].set_yscale('log')\n",
        "  axs[0].set_title(\"LR Scale\")\n",
        "  axs[0].set_ylabel(\"LR Scale\")\n",
        "  axs[0].set_xlabel(\"Epoch\")\n",
        "\n",
        "  axs[1].plot(train_losses[1:], lw=3)\n",
        "  axs[1].scatter(\n",
        "      jnp.argmin(jnp.array(train_losses)),\n",
        "      min(train_losses),\n",
        "      label=\"Min\",\n",
        "      s=100,\n",
        "  )\n",
        "  axs[1].set_title(\"Train loss\")\n",
        "  axs[1].set_xlabel(\"Epoch\")\n",
        "  axs[1].set_ylabel(\"Train Loss\")\n",
        "  axs[1].legend(frameon=False)\n",
        "\n",
        "  axs[2].plot(train_accuracy[1:], lw=3)\n",
        "  axs[2].scatter(\n",
        "      jnp.argmax(jnp.array(train_accuracy)),\n",
        "      max(train_accuracy),\n",
        "      label=\"Max\",\n",
        "      s=100,\n",
        "  )\n",
        "  axs[2].set_title(\"Train acc\")\n",
        "  axs[2].set_xlabel(\"Epoch\")\n",
        "  axs[2].set_ylabel(\"Train acc\")\n",
        "  axs[2].legend(frameon=False)\n",
        "\n",
        "  axs[3].plot(test_losses[1:], lw=3)\n",
        "  axs[3].scatter(\n",
        "      jnp.argmin(jnp.array(test_losses)),\n",
        "      min(test_losses),\n",
        "      label=\"Min\",\n",
        "      s=100,\n",
        "  )\n",
        "  axs[3].set_title(\"Test loss\")\n",
        "  axs[3].set_xlabel(\"Epoch\")\n",
        "  axs[3].set_ylabel(\"Test Loss\")\n",
        "  axs[3].legend(frameon=False)\n",
        "\n",
        "  axs[4].plot(test_accuracy[1:], lw=3)\n",
        "  axs[4].scatter(\n",
        "      jnp.argmax(jnp.array(test_accuracy)),\n",
        "      max(test_accuracy),\n",
        "      label=\"Max\",\n",
        "      s=100,\n",
        "  )\n",
        "  axs[4].set_title(\"Test acc\")\n",
        "  axs[4].set_ylabel(\"Test Acc\")\n",
        "  axs[4].legend(frameon=False)\n",
        "  axs[4].set_xlabel(\"Epoch\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbU6DkQwSSF0"
      },
      "outputs": [],
      "source": [
        "plot(lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNQgZChSSF0"
      },
      "source": [
        "## Reduce on average training loss\n",
        "\n",
        "Here we consider an implementation that reduces the learning rate according to an average training loss value agglomerated for some accumulation size hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8M8O97nTljZ"
      },
      "outputs": [],
      "source": [
        "# @markdown Number of epochs with no improvement after which learning rate will be reduced:\n",
        "PATIENCE = 5  # @param{type:\"integer\"}\n",
        "# @markdown Number of epochs to wait before resuming normal operation after the learning rate reduction:\n",
        "COOLDOWN = 0  # @param{type:\"integer\"}\n",
        "# @markdown Factor by which to reduce the learning rate:\n",
        "FACTOR = 0.5  # @param{type:\"number\"}\n",
        "# @markdown Relative tolerance for measuring the new optimum:\n",
        "RTOL = 1e-4  # @param{type:\"number\"}\n",
        "# @markdown Number of iterations to accumulate an average value:\n",
        "ACCUMULATION_SIZE = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHY9OEwZUa5Z"
      },
      "source": [
        "Here we chain the base optimizer and the reduce on plateau transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTQ5S69EUWor"
      },
      "outputs": [],
      "source": [
        "opt = optax.chain(\n",
        "    optax.adam(LEARNING_RATE),\n",
        "    contrib.reduce_on_plateau(\n",
        "        patience=PATIENCE,\n",
        "        cooldown=COOLDOWN,\n",
        "        factor=FACTOR,\n",
        "        rtol=RTOL,\n",
        "        accumulation_size=ACCUMULATION_SIZE,\n",
        "    ),\n",
        ")\n",
        "opt_state = opt.init(init_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5pi8ucoUs9A"
      },
      "source": [
        "In training step we feed the current value of the loss to the chained optimizer. This value is used to compute an average on ACCUMULATION_SIZE number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWXxsS7EUsOX"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(params, opt_state, batch):\n",
        "  \"\"\"Performs a one step update.\"\"\"\n",
        "  (value, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n",
        "      params, batch\n",
        "  )\n",
        "  updates, opt_state = opt.update(grad, opt_state, params, value=value)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, value, aux\n",
        "\n",
        "\n",
        "params = init_params\n",
        "\n",
        "# Computes metrics at initialization.\n",
        "train_stats = dataset_stats(params, test_loader_batched)\n",
        "train_accuracy = [train_stats[\"accuracy\"]]\n",
        "train_losses = [train_stats['loss']]\n",
        "\n",
        "test_stats = dataset_stats(params, test_loader_batched)\n",
        "test_accuracy = [test_stats[\"accuracy\"]]\n",
        "test_losses = [test_stats[\"loss\"]]\n",
        "\n",
        "lr_scale_history = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_accuracy_epoch = []\n",
        "  train_losses_epoch = []\n",
        "\n",
        "  for _, train_batch in enumerate(train_loader_batched.as_numpy_iterator()):\n",
        "    params, opt_state, train_loss, train_aux = train_step(\n",
        "        params, opt_state, train_batch\n",
        "    )\n",
        "    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n",
        "    train_losses_epoch.append(train_loss)\n",
        "\n",
        "  mean_train_accuracy = np.mean(train_accuracy_epoch)\n",
        "  mean_train_loss = np.mean(train_losses_epoch)\n",
        "\n",
        "  # fetch the scaling factor from the reduce_on_plateau transform\n",
        "  lr_scale = otu.tree_get(opt_state, \"scale\")\n",
        "  lr_scale_history.append(lr_scale)\n",
        "\n",
        "  train_accuracy.append(mean_train_accuracy)\n",
        "  train_losses.append(mean_train_loss)\n",
        "\n",
        "  test_stats = dataset_stats(params, test_loader_batched)\n",
        "  test_accuracy.append(test_stats[\"accuracy\"])\n",
        "  test_losses.append(test_stats[\"loss\"])\n",
        "  print(\n",
        "      f\"Epoch {epoch + 1}/{N_EPOCHS}, mean train accuracy:\"\n",
        "      f\" {mean_train_accuracy}, lr scale: {otu.tree_get(opt_state, 'scale')}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30CB2vGAW9Nd"
      },
      "outputs": [],
      "source": [
        "plot(lr_scale_history, train_losses, train_accuracy, test_losses, test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rlayCzLXA58"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "provenance": []
    },
    "execution": {
      "timeout": -1
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
