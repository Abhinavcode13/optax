{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optax 101",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQz7Vp8ehqb"
      },
      "source": [
        "# Learn Optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKcocLxEyYf2"
      },
      "source": [
        "## Quick Start\n",
        "\n",
        "Let's use optax to fit a parametrized function. We will consider the problem of learning to identify when a value is odd or even.\n",
        "\n",
        "We will begin by creating a dataset that consists of batches of random 8 bit integers (represented using their binary representation), with each value labelled as \"odd\" or \"even\" using 1-hot encoding (i.e. `[1, 0]` means odd `[0, 1]` means even).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg6zyMBqydty"
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "NUM_TRAIN_STEPS = 1_000\n",
        "RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n",
        "\n",
        "TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n",
        "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV79rjQK8tvC"
      },
      "source": [
        "We may now define a parametrized function using JAX. This will allow us to efficiently compute gradients.\n",
        "\n",
        "There are a number of libraries that provide common building blocks for parametrized functions (such as flax and haiku). For this case though, we shall implement our function from scratch.\n",
        "\n",
        "Our function will be a 1-layer MLP (multi-layer perceptron) with a single hidden layer, and a single output layer. We initialize all parameters using a standard Gaussian $\\mathcal{N}(0,1)$ distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syp9LJ338h9-"
      },
      "source": [
        "initial_params = {\n",
        "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
        "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
        "}\n",
        "\n",
        "\n",
        "def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:\n",
        "  x = jnp.dot(x, params['hidden'])\n",
        "  x = jax.nn.relu(x)\n",
        "  x = jnp.dot(x, params['output'])\n",
        "  return x\n",
        "\n",
        "\n",
        "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
        "  y_hat = net(batch, params)\n",
        "\n",
        "  # optax also provides a number of common loss functions.\n",
        "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
        "\n",
        "  return loss_value.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LVHrJyH9vDe"
      },
      "source": [
        "We will use `optax.adam` to compute the parameter updates from their gradients on each optimizer step.\n",
        "\n",
        "Note that since optax optimizers are implemented using pure functions, we will need to also keep track of the optimizer state. For the Adam optimizier, this state will contain the momentum values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsbPBTF09FGY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1636155226542,
          "user_tz": 0,
          "elapsed": 6046,
          "user": {
            "displayName": "Ross Hemsley",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64",
            "userId": "11415908946302743815"
          }
        },
        "outputId": "c427f94f-a605-44fc-b519-707bc5d47b7d"
      },
      "source": [
        "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  @jax.jit\n",
        "  def step(params, opt_state, batch, labels):\n",
        "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss_value\n",
        "\n",
        "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
        "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
        "    if i % 100 == 0:\n",
        "      print(f'step {i}, loss: {loss_value}')\n",
        "\n",
        "  return params\n",
        "\n",
        "# Finally, we can fit our parametrized function using the Adam optimizier\n",
        "# provided by optax.\n",
        "optimizer = optax.adam(learning_rate=1e-2)\n",
        "params = fit(initial_params, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss: 5.60183048248291\n",
            "step 100, loss: 0.14773361384868622\n",
            "step 200, loss: 0.28999248147010803\n",
            "step 300, loss: 0.05951451137661934\n",
            "step 400, loss: 0.08592046797275543\n",
            "step 500, loss: 0.005035111214965582\n",
            "step 600, loss: 0.0028563595842570066\n",
            "step 700, loss: 0.013286210596561432\n",
            "step 800, loss: 0.01311601884663105\n",
            "step 900, loss: 0.003692328929901123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTaBLYL8_Ppz"
      },
      "source": [
        "We see that our loss appears to have converged, which should indicate that we have successfully found better parameters for our network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT_Uaei5Dv_3"
      },
      "source": [
        "## Weight Decay, Schedules and Clipping\n",
        "\n",
        "Many research models make use of techniques such as learning rate scheduling, and gradient clipping. These may be achieved by _chaining_ together gradient transformations such as `optax.adam` and `optax.clip`.\n",
        "\n",
        "In the following, we will use `Adam` with weight decay (`optax.adamw`), a cosine learning rate schedule (with warmup) and also gradient clipping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZegYQajDtLi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1636155227388,
          "user_tz": 0,
          "elapsed": 734,
          "user": {
            "displayName": "Ross Hemsley",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64",
            "userId": "11415908946302743815"
          }
        },
        "outputId": "f65f9fd8-8e9c-4ae6-e759-62362ff94f53"
      },
      "source": [
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "  init_value=0.0,\n",
        "  peak_value=1.0,\n",
        "  warmup_steps=50,\n",
        "  decay_steps=1_000,\n",
        "  end_value=0.0,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "  optax.clip(1.0),\n",
        "  optax.adamw(learning_rate=schedule),\n",
        ")\n",
        "\n",
        "params = fit(initial_params, optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, loss: 5.60183048248291\n",
            "step 100, loss: 1.0181801179953709e-08\n",
            "step 200, loss: 0.27725887298583984\n",
            "step 300, loss: 0.0\n",
            "step 400, loss: 0.0\n",
            "step 500, loss: 0.0\n",
            "step 600, loss: 0.0\n",
            "step 700, loss: 0.0\n",
            "step 800, loss: 0.0\n",
            "step 900, loss: 0.0\n"
          ]
        }
      ]
    }
  ]
}
