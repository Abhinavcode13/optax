{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Warlord-K/optax/blob/master/docs/optax-101.ipynb","timestamp":1665052922763}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EXQz7Vp8ehqb"},"source":["# Learn Optax"]},{"cell_type":"markdown","metadata":{"id":"tKcocLxEyYf2"},"source":["## Quick Start\n","\n","Let's use optax to fit a parametrized function. We will consider the problem of learning to identify when a value is odd or even.\n","\n","We will begin by creating a dataset that consists of batches of random 8 bit integers (represented using their binary representation), with each value labelled as \"odd\" or \"even\" using 1-hot encoding (i.e. `[1, 0]` means odd `[0, 1]` means even).\n"]},{"cell_type":"code","metadata":{"id":"Gg6zyMBqydty"},"source":["import random\n","from typing import Tuple\n","\n","import optax\n","import jax.numpy as jnp\n","import jax\n","import numpy as np\n","\n","BATCH_SIZE = 5\n","NUM_TRAIN_STEPS = 1_000\n","RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n","\n","TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n","LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nV79rjQK8tvC"},"source":["We may now define a parametrized function using JAX. This will allow us to efficiently compute gradients.\n","\n","There are a number of libraries that provide common building blocks for parametrized functions (such as flax and haiku). For this case though, we shall implement our function from scratch.\n","\n","Our function will be a 1-layer MLP (multi-layer perceptron) with a single hidden layer, and a single output layer. We initialize all parameters using a standard Gaussian $\\mathcal{N}(0,1)$ distribution."]},{"cell_type":"code","metadata":{"id":"Syp9LJ338h9-"},"source":["initial_params = {\n","    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n","    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n","}\n","\n","\n","def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:\n","  x = jnp.dot(x, params['hidden'])\n","  x = jax.nn.relu(x)\n","  x = jnp.dot(x, params['output'])\n","  return x\n","\n","\n","def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n","  y_hat = net(batch, params)\n","\n","  # optax also provides a number of common loss functions.\n","  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n","\n","  return loss_value.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2LVHrJyH9vDe"},"source":["We will use `optax.adam` to compute the parameter updates from their gradients on each optimizer step.\n","\n","Note that since optax optimizers are implemented using pure functions, we will need to also keep track of the optimizer state. For the Adam optimizer, this state will contain the momentum values."]},{"cell_type":"code","metadata":{"id":"JsbPBTF09FGY","executionInfo":{"status":"ok","timestamp":1636155226542,"user_tz":0,"elapsed":6046,"user":{"displayName":"Ross Hemsley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64","userId":"11415908946302743815"}},"outputId":"c427f94f-a605-44fc-b519-707bc5d47b7d"},"source":["def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n","  opt_state = optimizer.init(params)\n","\n","  @jax.jit\n","  def step(params, opt_state, batch, labels):\n","    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","    return params, opt_state, loss_value\n","\n","  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n","    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n","    if i % 100 == 0:\n","      print(f'step {i}, loss: {loss_value}')\n","\n","  return params\n","\n","# Finally, we can fit our parametrized function using the Adam optimizer\n","# provided by optax.\n","optimizer = optax.adam(learning_rate=1e-2)\n","params = fit(initial_params, optimizer)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0, loss: 5.60183048248291\n","step 100, loss: 0.14773361384868622\n","step 200, loss: 0.28999248147010803\n","step 300, loss: 0.05951451137661934\n","step 400, loss: 0.08592046797275543\n","step 500, loss: 0.005035111214965582\n","step 600, loss: 0.0028563595842570066\n","step 700, loss: 0.013286210596561432\n","step 800, loss: 0.01311601884663105\n","step 900, loss: 0.003692328929901123\n"]}]},{"cell_type":"markdown","metadata":{"id":"kTaBLYL8_Ppz"},"source":["We see that our loss appears to have converged, which should indicate that we have successfully found better parameters for our network"]},{"cell_type":"markdown","metadata":{"id":"qT_Uaei5Dv_3"},"source":["## Weight Decay, Schedules and Clipping\n","\n","Many research models make use of techniques such as learning rate scheduling, and gradient clipping. These may be achieved by _chaining_ together gradient transformations such as `optax.adam` and `optax.clip`.\n","\n","In the following, we will use `Adam` with weight decay (`optax.adamw`), a cosine learning rate schedule (with warmup) and also gradient clipping."]},{"cell_type":"code","metadata":{"id":"SZegYQajDtLi","executionInfo":{"status":"ok","timestamp":1636155227388,"user_tz":0,"elapsed":734,"user":{"displayName":"Ross Hemsley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjSZqBnQizDvVofyb2N_r9W3cP4duk9mv1mxCb9=s64","userId":"11415908946302743815"}},"outputId":"f65f9fd8-8e9c-4ae6-e759-62362ff94f53"},"source":["schedule = optax.warmup_cosine_decay_schedule(\n","  init_value=0.0,\n","  peak_value=1.0,\n","  warmup_steps=50,\n","  decay_steps=1_000,\n","  end_value=0.0,\n",")\n","\n","optimizer = optax.chain(\n","  optax.clip(1.0),\n","  optax.adamw(learning_rate=schedule),\n",")\n","\n","params = fit(initial_params, optimizer)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0, loss: 5.60183048248291\n","step 100, loss: 1.0181801179953709e-08\n","step 200, loss: 0.27725887298583984\n","step 300, loss: 0.0\n","step 400, loss: 0.0\n","step 500, loss: 0.0\n","step 600, loss: 0.0\n","step 700, loss: 0.0\n","step 800, loss: 0.0\n","step 900, loss: 0.0\n"]}]},{"cell_type":"markdown","source":["##Components\n","\n","We refer to the [docs](https://optax.readthedocs.io/en/latest/index.html) for a detailed list of available Optax components. Here, we highlight the main categories of buiilding blocks provided by Optax."],"metadata":{"id":"qf53Y6mT1Vwl"}},{"cell_type":"markdown","source":["### Gradient Transformations [(transform.py)](https://github.com/deepmind/optax/blob/master/optax/_src/transform.py)\n","\n","One of the key building blocks of optax is a GradientTransformation.\n","\n","Each transformation is defined two functions:\n","\n","state = init(params)\n","\n","grads, state = update(grads, state, params=None)\n","\n","The init function initializes a (possibly empty) set of statistics (aka state) and the update function transforms a candidate gradient given some statistics, and (optionally) the current value of the parameters.\n","\n","For example:"],"metadata":{"id":"WZFpEKi82TGx"}},{"cell_type":"code","source":["tx = scale_by_rms()\n","state = tx.init(params)  # init stats\n","grads, state = tx.update(grads, state, params)  # transform & update stats."],"metadata":{"id":"_yCQbSCc2KhJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Composing Gradient Transformations [(combine.py)](https://github.com/deepmind/optax/blob/master/optax/_src/combine.py)\n","\n","The fact that transformations take candidate gradients as input and return processed gradients as output (in contrast to returning the updated parameters) is critical to allow to combine arbitrary transformations into a custom optimiser / gradient processor, and also allows to combine transformations for different gradients that operate on a shared set of variables.\n","\n","For instance, chain combines them sequentially, and returns a new GradientTransformation that applies several transformations in sequence.\n","\n","For example:"],"metadata":{"id":"TyxJmbBq2xT6"}},{"cell_type":"code","source":["my_optimiser = chain(\n","    clip_by_global_norm(max_norm),\n","    scale_by_adam(eps=1e-4),\n","    scale(-learning_rate))"],"metadata":{"id":"TNPC9e7I28m8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Wrapping Gradient Transformations [(wrappers.py)](https://github.com/deepmind/optax/blob/master/optax/_src/wrappers.py)\n","\n","Optax also provides several wrappers that take a GradientTransformation as input and return a new GradientTransformation that modifies the behaviour of the inner transformation in a specific way.\n","\n","For instance the flatten wrapper flattens gradients into a single large vector before applying the inner GradientTransformation. The transformed updated are then unflattened before being returned to the user. This can be used to reduce the overhead of performing many calculations on lots of small variables, at the cost of increasing memory usage.\n","\n","For example:"],"metadata":{"id":"JmV92-PI2_pS"}},{"cell_type":"code","source":["my_optimiser = flatten(adam(learning_rate))"],"metadata":{"id":"b1TlMbAk3Jbo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Other examples of wrappers include accumulating gradients over multiple steps, or applying the inner transformation only to specific parameters or at specific steps."],"metadata":{"id":"IUCIMymV3M2n"}},{"cell_type":"markdown","source":["###Schedules [(schedule.py)](https://github.com/deepmind/optax/blob/master/optax/_src/schedule.py)\n","\n","Many popular transformations use time dependent components, e.g. to anneal some hyper-parameter (e.g. the learning rate). Optax provides for this purpose schedules that can be used to decay scalars as a function of a step count.\n","\n","For example you may use a polynomial schedule (with power=1) to decay a hyper-parameter linearly over a number of steps:"],"metadata":{"id":"AGAmqST33PkO"}},{"cell_type":"code","source":["schedule_fn = polynomial_schedule(\n","    init_value=1., end_value=0., power=1, transition_steps=5)\n","\n","for step_count in range(6):\n","  print(schedule_fn(step_count))  # [1., 0.8, 0.6, 0.4, 0.2, 0.]"],"metadata":{"id":"Zbr61DLP3ecy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Schedules are used by certain gradient transformation, for instance:"],"metadata":{"id":"LGt0AzHF3fjR"}},{"cell_type":"code","source":["schedule_fn = polynomial_schedule(\n","    init_value=-learning_rate, end_value=0., power=1, transition_steps=5)\n","optimiser = chain(\n","    clip_by_global_norm(max_norm),\n","    scale_by_adam(eps=1e-4),\n","    scale_by_schedule(schedule_fn))"],"metadata":{"id":"W9oCb0Kw3igG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Popular optimisers [(alias.py)](https://github.com/deepmind/optax/blob/master/optax/_src/alias.py)\n","\n","In addition to the low level building blocks we also provide aliases for popular optimisers built using these components (e.g. RMSProp, Adam, AdamW, etc, ...). These are all still instances of a GradientTransformation, and can therefore be further combined with any of the individual building blocks.\n","\n","For example:"],"metadata":{"id":"cKHZrM203kx4"}},{"cell_type":"code","source":["def adamw(learning_rate, b1, b2, eps, weight_decay):\n","  return chain(\n","      scale_by_adam(b1=b1, b2=b2, eps=eps),\n","      scale_and_decay(-learning_rate, weight_decay=weight_decay))"],"metadata":{"id":"Czk49AQz3w1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Applying updates [(update.py)](https://github.com/deepmind/optax/blob/master/optax/_src/update.py)\n","\n","After transforming an update using a GradientTransformation or any custom manipulation of the update, you will typically apply the update to a set of parameters. This can be done trivially using tree_map.\n","\n","For convenience, we expose an apply_updates function to apply updates to parameters. The function just adds the updates and the parameters together, i.e. tree_map(lambda p, u: p + u, params, updates)."],"metadata":{"id":"j0tD_jWC3zar"}},{"cell_type":"code","source":["updates, state = tx.update(grads, state, params)  # transform & update stats.\n","new_params = optax.apply_updates(params, updates)  # update the parameters."],"metadata":{"id":"YG-TNzYm4CHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that separating gradient transformations from the parameter update is critical to support composing sequence of transformations (e.g. chain), as well as combine multiple updates to the same parameters (e.g. in multi-task settings where different tasks need different sets of gradient transformations)."],"metadata":{"id":"eg85y6_s4C2c"}},{"cell_type":"markdown","source":["###Losses [(loss.py)](https://github.com/deepmind/optax/blob/master/optax/_src/loss.py)\n","\n","Optax provides a number of standard losses used in deep learning, such as l2_loss, softmax_cross_entropy, cosine_distance, etc."],"metadata":{"id":"dJzW0Flw4FP5"}},{"cell_type":"code","source":["loss = huber_loss(predictions, targets)"],"metadata":{"id":"8JCWgHhJ4PMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The losses accept batches as inputs, however they perform no reduction across the batch dimension(s). This is trivial to do in JAX, for example:"],"metadata":{"id":"gAlaEpgQ4QyD"}},{"cell_type":"code","source":["avg_loss = jnp.mean(huber_loss(predictions, targets))\n","sum_loss = jnp.sum(huber_loss(predictions, targets))"],"metadata":{"id":"45svU6Qr4ThD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Second Order [(second_order.py)](https://github.com/deepmind/optax/blob/master/optax/_src/second_order.py)\n","\n","Computing the Hessian or Fisher information matrices for neural networks is typically intractable due to the quadratic memory requirements. Solving for the diagonals of these matrices is often a better solution. The library offers functions for computing these diagonals with sub-quadratic memory requirements."],"metadata":{"id":"MepQR-Cr4VaB"}},{"cell_type":"markdown","source":["###Stochastic gradient estimators [(stochastic_gradient_estimators.py)](https://github.com/deepmind/optax/blob/master/optax/_src/stochastic_gradient_estimators.py)\n","\n","Stochastic gradient estimators compute Monte Carlo estimates of gradients of the expectation of a function under a distribution with respect to the distribution's parameters.\n","\n","Unbiased estimators, such as the score function estimator (REINFORCE), pathwise estimator (reparameterization trick) or measure valued estimator, are implemented: score_function_jacobians, pathwise_jacobians and measure_valued_jacobians. Their applicability (both in terms of functions and distributions) is discussed in their respective documentation.\n","\n","Stochastic gradient estimators can be combined with common control variates for variance reduction via control_variates_jacobians. For provided control variates see delta and moving_avg_baseline.\n","\n","The result of a gradient estimator or control_variates_jacobians contains the Jacobians of the function with respect to the samples from the input distribution. These can then be used to update distributional parameters, or to assess gradient variance.\n","\n","Example of how to use the pathwise_jacobians estimator:"],"metadata":{"id":"fcJiCWSQ4gPP"}},{"cell_type":"code","source":["dist_params = [mean, log_scale]\n","function = lambda x: jnp.sum(x * weights)\n","jacobians = pathwise_jacobians(\n","      function, dist_params,\n","      utils.multi_normal, rng, num_samples)\n","\n","mean_grads = jnp.mean(jacobians[0], axis=0)\n","log_scale_grads = jnp.mean(jacobians[1], axis=0)\n","grads = [mean_grads, log_scale_grads]\n","optim_update, optim_state = optim.update(grads, optim_state)\n","updated_dist_params = optax.apply_updates(dist_params, optim_update)"],"metadata":{"id":"NYJOV6Vv4uDb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["where optim is an optax optimizer."],"metadata":{"id":"x5mXAl_H4xCH"}}]}